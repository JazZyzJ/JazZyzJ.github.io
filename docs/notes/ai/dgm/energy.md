---
comment: true
---


# Energy-based Models

<head>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.js"
            integrity="sha512-EKW5YvKU3hpyyOcN6jQnAxO/L8gts+YdYV6Yymtl8pk9YlYFtqJgihORuRoBXK8/cOIlappdU6Ms8KdK6yBCgA=="
            crossorigin="anonymous" referrerpolicy="no-referrer">
    </script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
    <script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js">
    </script>
</head>

## Intro

EBMs å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

- Flexible model arch
- Stable training
- Flexible composition

è¿™ç‚¹ä¸ä¹‹å‰çš„ä¸¤ç±»æ¨¡å‹ä¸åŒï¼š

è¿›è¡Œä¸€ä¸ªå¤§ä½“ä¸Šçš„åŒºåˆ†çš„è¯ï¼Œå…ˆå‰å­¦ä¹ çš„

- VAEã€Flowã€ARéƒ½å±äºåŸºäºä¼¼ç„¶çš„æ¨¡å‹ï¼Œä»–ä»¬çš„ç‰¹ç‚¹æ˜¯å…·æœ‰ä¸¥æ ¼çš„æ¦‚ç‡è§£é‡Šå’Œè®­ç»ƒç›®æ ‡ï¼Œä½†æ˜¯æ¨¡å‹ç»“æ„ç›¸å¯¹å›ºå®šï¼Œéš¾ä»¥çµæ´»è°ƒæ•´ã€‚

- GANså±äºimplicit modelsï¼Œä»–ä¸å…·æœ‰æ¦‚ç‡è§£é‡Šï¼Œè¿™æ ·ä½¿å¾—æ¨¡å‹çš„æ¶æ„æ›´åŠ çµæ´»ï¼Œä½†åŒæ—¶è®­ç»ƒä¸å¤ªæ–¹ä¾¿

EBMsä¹Ÿæ˜¯åŸºäºä¼¼ç„¶çš„æ¨¡å‹

### Recap

å›å¿†æˆ‘ä»¬ä¹‹å‰å­¦ä¹ çš„æ¦‚ç‡æ¨¡å‹ï¼Œæˆ‘ä»¬æ„å»ºçš„pdfæœ‰ä»¥ä¸‹ä¸¤ç‚¹è¦æ±‚ï¼š

- Non-negative
- Sum to 1

é€šå¸¸æ¥è¯´æ„å»ºä¸€ä¸ªéè´Ÿçš„pdfæ˜¯å®¹æ˜“çš„ï¼Œç”¨å¹³æ–¹ã€æŒ‡æ•°ã€ç»å¯¹å€¼éƒ½å¯ä»¥çš„ï¼Œä½†æ˜¯å½’ä¸€åŒ–å¹¶ä¸ç®€å•ï¼Œéœ€è¦è®¡ç®—ç§¯åˆ†

???+ note "Common usage"

    - $g_{\mu, \sigma}(x) = e^{-\frac{(x-\mu)^2}{2\sigma^2}} \rightarrow \sqrt{2\pi\sigma^2}$

    - $g_{\lambda}(x) = e^{-\lambda x} \rightarrow \frac{1}{\lambda}$
    
    - Exponential family: Normal, Poisson, beta, gamma, etc
        - $g_{\theta}(x) = h(x) \exp(\theta \cdot T(x)) \rightarrow \exp{A(\theta)}$ where $A(\theta) = \log \int h(x) \exp(\theta \cdot T(x)) dx$

åœ¨è¿›è¡Œå½’ä¸€åŒ–æ“ä½œåï¼Œæˆ‘ä»¬ç»„åˆè¿™äº›å‡½æ•°è¿›è¡Œå»ºæ¨¡ï¼Œå°±æœ‰äº†ï¼š

<div align="center" >
    <img src="/../../../../assets/pics/ai/dgm/energy/e1.png" style="width: 90%;">
    </div>

å¯ä»¥çœ‹åˆ°è¿™äº›æ¨¡å‹éƒ½æ˜¯å°†normalization constantç¡®å®šä¸º1ï¼Œè€Œåœ¨EBMsä¸­ï¼Œæˆ‘ä»¬ä¸ä»…ä¸ä¼šæ§åˆ¶è¿™ä¸ªå¸¸æ•°ï¼Œç”šè‡³ä¸ä¼šç²¾ç¡®ç®—å‡ºæ¥å®ƒ

## Build

$$
p_{\theta}(x) = \frac{1}{Z(\theta)} e^{f_\theta(x)}
$$

$Z(\theta) = \int e^{f_\theta(x)} dx$ is called the **Partition Function**

å¯ä»¥çœ‹åˆ°æˆ‘ä»¬ä½¿ç”¨äº†æŒ‡æ•°å½¢å¼ï¼Œæœ‰ä»¥ä¸‹åŸå› ï¼š

- å¯ä»¥æ•è·å·¨å¤§çš„æ¦‚ç‡å˜åŒ–ï¼Œå¹¶ä¸”ä¸€èˆ¬æ˜¯ä½¿ç”¨log-probability
- å¾ˆå¤šçš„æ¦‚ç‡åˆ†å¸ƒéƒ½æ˜¯æŒ‡æ•°æ—çš„
- æ»¡è¶³ä¸€äº›ç‰©ç†å®šå¾‹
    - å®é™…ä¸Šenergyè¿™ä¸ªåå­—çš„æ¥æºå°±æ˜¯ç‰©ç†å­¦ï¼ˆæœ€å¤§ç†µã€çƒ­äºŒå®šå¾‹ï¼‰ï¼Œåœ¨è¿™é‡Œå°±æ˜¯æŒ‡$-f_\theta(x)$



**Pros:** extreme flexibility(any function $f_\theta$ can be used)

**Cons:** 

- Zå¦‚æœéš¾ä»¥è®¡ç®—ï¼Œé‚£ä¹ˆsamplingå°±å¾ˆéš¾è¿›è¡Œ
- No feature learning
- Curse of dimensionality



## Application

è™½ç„¶è¿›è¡Œå•ä¸ªæ ·æœ¬çš„é‡‡æ ·å¾ˆéš¾ï¼Œä½†æ˜¯å¯¹äºä¸¤ä¸ªæ ·æœ¬ç‚¹çš„æ¯”è¾ƒä¼šæ¯”è¾ƒå®¹æ˜“ï¼Œå› ä¸ºåšæ¯”å€¼åï¼ŒZå°±çº¦å»äº†ï¼š

$$
\frac{p_\theta(x)}{p_\theta(x')} = exp(f_\theta(x) - f_\theta(x'))
$$

<div align="center" >
    <img src="/../../../../assets/pics/ai/dgm/energy/e2.png" style="width: 90%;">
    </div>


## Training

åœ¨å‰é¢çš„å†…å®¹ä¸­æˆ‘ä»¬è®¨è®ºäº†EBMsçš„å®šä¹‰ï¼Œä»¥åŠåˆ©ç”¨EBMsè¿›è¡Œä¸¤ä¸ªæ ·æœ¬ç‚¹æ¯”è¾ƒï¼Œä½†è¿™æ˜¯å»ºç«‹åœ¨å·²ç»æœ‰ä¸€ä¸ªè®­ç»ƒå¥½çš„EBMåŸºç¡€ä¸Šï¼Œç°åœ¨æˆ‘ä»¬çš„é—®é¢˜æ˜¯å¦‚ä½•è®­ç»ƒä¸€ä¸ªEBMï¼Œä¹Ÿå°±æ˜¯maximize $p_\theta(x)$

ä¸€ä¸ªç›´è§‚çš„æ„Ÿå—å°±æ˜¯ï¼Œå¯¹äºè¿™æ ·çš„åˆ†æ•°ï¼Œå¦‚æœæˆ‘ä»¬èƒ½å¢å¤§åˆ†å­ï¼Œå‡å°åˆ†æ¯ï¼Œé‚£ä¹ˆåˆ†æ•°å€¼å°±ä¼šå¢å¤§ï¼Œè¿™å°±æ˜¯ **Contrastive Divergence**

$$
\max_\theta \nabla_\theta (f_\theta(x_{train}) - f_\theta(x_{sample}))
$$

??? quote "Prove"

    <div align="center" >
    <img src="/../../../../assets/pics/ai/dgm/energy/e3.png" style="width: 90%;">
    </div>

    æœ€åçš„é‚£æ­¥è¿‘ä¼¼å°±æ˜¯sampleçš„MCè¿‘ä¼¼

## Sampling

ç°åœ¨çš„é—®é¢˜æ¥åˆ°æˆ‘ä»¬åº”è¯¥å¦‚ä½•sampleï¼š

å› ä¸ºæˆ‘ä»¬æ²¡æœ‰åŠæ³•è®¡ç®—$Z$ï¼Œæ‰€ä»¥è¿˜æ˜¯é‡‡å–æ¯”è¾ƒçš„æ€æƒ³ï¼š



<pre class="pseudocode">
    \begin{algorithm}
    \caption{Markov Chain Monte Carlo(MCMC) for EBM}
    \begin{algorithmic}
    \REQUIRE initial parameters
    \STATE Initialize $x^0$ randomly, set $t = 0$
    \REPEAT 
    \STATE $x' \gets x^t + \text{noise}$ \COMMENT{Perturb current solution}
    \IF{$f_\theta(x') > f_\theta(x^t)$}
        \STATE $x^{t+1} \gets x'$ \COMMENT{Always accept better solutions}
    \ELSE
        \STATE $\text{Accept } x^{t+1} = x' \text{ with probability } \exp(f_\theta(x') - f_\theta(x^t))$
    \ENDIF
        \STATE $t \gets t + 1$ 
    \UNTIL{t = T} 
    
    \end{algorithmic}
    \end{algorithm}
</pre>


è¿™æœ‰ç‚¹åƒæ¨¡æ‹Ÿé€€ç«ï¼Œä¸ºäº†é¿å…åœç•™åœ¨ä¸€ä¸ªlocalï¼Œæˆ‘ä»¬å‚è€ƒäº†ä¸€ä¸€ä¸ªæ¦‚ç‡é€‰æ‹©é‚£ä¸ªè¾ƒå·®çš„sample

- ç†è®ºä¸Šæ¥è¯´$x^T$ä¼šæ”¶æ•›åˆ°$p_\theta(x)$(åœ¨$T \rightarrow \infty$æ—¶)
    - å› ä¸ºä»–æ»¡è¶³ä¸€ä¸ªå«**detailed balance**çš„æ¡ä»¶ï¼šå¯ä»¥çœ‹åˆ°è¿™ä¸ªå¼å­åœ¨xè¾¾åˆ°$p_\theta(x)$æ—¶ï¼Œx'ä¸ä¼šå†è¿›è¡Œæ›´æ–°

    $$
    p_\theta(x) \cdot T(x, x') = p_\theta(x') \cdot T(x', x)
    $$

    è¿™é‡Œ$T(x', x)$æ˜¯æŒ‡xåˆ°x'çš„transitioningæ¦‚ç‡

- å®é™…ä¸­ï¼Œéœ€è¦ä¸€ä¸ªå¤§é‡çš„å¾ªç¯æ‰èƒ½è¾¾åˆ°æ”¶æ•›
    
    ä¹Ÿå°±æ˜¯è¯´ï¼Œå³ä½¿æˆ‘ä»¬æ‹¥æœ‰ä¸€ä¸ªå¾ˆå¥½çš„$f_\theta$ï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦å¤§é‡çš„ç²¾åŠ›å»sampleï¼Œå¹¶ä¸”æˆ‘ä»¬çœ‹åˆ°ï¼Œä½¿ç”¨contrasive divergenceè¿›è¡Œçš„è®­ç»ƒçš„æ—¶å€™æœ¬èº«å°±æ˜¯ä¸€ä¸ªsampleçš„è¿‡ç¨‹ï¼Œæ‰€ä»¥å³ä½¿æ˜¯è®­ç»ƒä¹Ÿæ˜¯ååˆ†æ˜‚è´µçš„

ä¸€ä¸ªæ”¹è¿›ï¼š


<pre class="pseudocode">
    \begin{algorithm}
    \caption{unadjusted Langevin MCMC}
    \begin{algorithmic}
    \REQUIRE initial parameters
    \STATE Initialize $x^0 \sim \pi(x)$ randomly, set $t = 0$
    \REPEAT
        \STATE Add random noise $z \sim N(0, 1)$
        \STATE $x^{t+1} \gets x^t + \epsilon \nabla_{x} \log p_\theta(x^t) + \sqrt{2 \epsilon} z^t$
    \UNTIL{t = T}
    \end{algorithmic}
    \end{algorithm}
</pre>

è¿™ä¸ªæœ—ä¹‹ä¸‡åŠ¨åŠ›ç®—æ³•çš„æ€è·¯å’Œä¹‹å‰çš„å·®ä¸å¤šï¼Œå°±æ˜¯æ²¿ç€æœ‰å™ªå£°çš„å¯¹æ•°ä¼¼ç„¶æ¢¯åº¦ä¸Šå‡æ–¹å‘æ¥ç”Ÿæˆ

- å¯ä»¥åœ¨Tè¶‹å‘âˆå’Œ$\epsilon \rightarrow 0$æ—¶æ”¶æ•›åˆ°$p_\theta(x)$
- å¯¹äºè¿ç»­çš„EBMsï¼Œ$\nabla_{x} \log p_\theta(x) = \nabla_{x} f_\theta(x)$

è¿™ä¸ªæ–¹æ³•çš„å¥½å¤„åœ¨äºæˆ‘ä»¬ç°åœ¨ä¸æ˜¯éšæœºæ¸¸èµ°ï¼Œè€Œæ˜¯åœ¨æ¢¯åº¦æ–¹å‘ä¸Šè¿›è¡Œç§»åŠ¨


## Better Training


è™½ç„¶æˆ‘ä»¬å¾—åˆ°äº†ä¸€äº›æ¯”è¾ƒå¥½çš„sampleæ–¹å¼ï¼Œä½†æ˜¯åœ¨è®­ç»ƒæ—¶å¦‚æœè¿˜æ˜¯ä½¿ç”¨å†…éƒ¨å¾ªç¯çš„è¿™ç§æ–¹å¼ï¼Œä¾ç„¶æ˜¯æ— æ³•æ¥å—çš„ï¼Œä¸‹é¢æå‡ºä¸€äº›æ–°çš„è®­ç»ƒæ–¹æ³•


### Score-based Models

å›é¡¾ä¸€ä¸‹energy-basedï¼š

$$
p_\theta(x) = \frac{exp{f_\theta(x)}}{Z(\theta)}, \text{ and } \log p_\theta(x) = f_\theta(x) - \log Z(\theta)
$$

ç°åœ¨ç»™å‡ºscore functionï¼š

$$
s_\theta(x) = \nabla_{x} \log p_\theta(x) = \nabla_{x} f_\theta(x)
$$

è¿™é‡Œå’Œä¹‹å‰æœ—ä¹‹ä¸‡é‚£è¾¹ä¸€æ ·ï¼Œæ²¡æœ‰Zæ˜¯å› ä¸ºå¯¹xæ±‚æ¢¯åº¦åï¼ŒZå°±æ˜¯0äº†ï¼Œæ‰€ä»¥å¯¹äºå½’ä¸€åŒ–å¸¸æ•°ï¼Œæˆ‘ä»¬çš„score functionæœ‰å¾ˆå¥½çš„æ•ˆæœï¼š

<div align="center" >
    <img src="/../../../../assets/pics/ai/dgm/energy/e4.png" style="width: 40%;">
    </div>

è¿™æ ·æˆ‘ä»¬å°±ä¸ç”¨æ±‚partition functionäº†ï¼Œæœ¬è´¨ä¸Šæˆ‘ä»¬æ²¡æœ‰è¿›è¡Œæ”¹å˜ï¼Œä½†æ˜¯é‡æ–°æ¢äº†ä¸€ç§è®¡ç®—æ–¹å¼ï¼Œå› ä¸ºåŸºäºæ¢¯åº¦çš„æ–¹æ³•ä¹Ÿå¯ä»¥ä»å¦ä¸€ä¸ªè§’åº¦è¿›è¡Œä¸¤ä¸ªæ¦‚ç‡ç‚¹çš„æ¯”è¾ƒï¼Œè€Œä¸æ˜¯åœ¨partition functionä¸­æ¯”è¾ƒå æ¯”å¤§å°


ç°åœ¨æˆ‘ä»¬æå‡ºè®­ç»ƒç›®æ ‡å‡½æ•°ï¼š

**Score Matching**ï¼šminimize the Fisher divergence between $p_{data}$ and the EBM $p_\theta \propto \exp{f_\theta(x)}$

$$
\frac{1}{2} \mathbb{E}_{x \sim p_{data}} \| \nabla_{x} \log p_{data}(x) - s_\theta(x) \|_{2}^2
$$

ç°åœ¨çš„é—®é¢˜æ˜¯ï¼šæˆ‘ä»¬æ²¡æœ‰$p_{data}$ï¼ˆæœ‰äº†è¿˜è®­ç»ƒä¸ªğŸ¥šï¼‰

å¥½æ¶ˆæ¯æ˜¯æˆ‘ä»¬å¯ä»¥ç”¨åˆ†éƒ¨ç§¯åˆ†æŠŠ$p_{data}$ç»™æ¶ˆæ‰ï¼š

???+ quote "Prove in 1 dimension"
    
    <div align="center" >
    <img src="/../../../../assets/pics/ai/dgm/energy/e5.png" style="width: 90%;">
    </div>

    <div align="center" >
    <img src="/../../../../assets/pics/ai/dgm/energy/e6.png" style="width: 90%;">
    </div>

    åœ¨è¿™ç§ä¸€ç»´æƒ…å†µçš„è¯æ˜ä¸­ï¼Œnablaç®—å­å°±æ˜¯æ±‚å¯¼ï¼Œåœ¨è¿›è¡Œç§¯åˆ†æ—¶ï¼Œ$p_{data}$åœ¨æ­£è´Ÿæ— ç©·å¤„ä¸º0ï¼Œæ‰€ä»¥å¯ä»¥æ¶ˆå»


æœ€ç»ˆæˆ‘ä»¬å¾—åˆ°çš„ç»“æœæ˜¯ï¼š

$$
=E_{\mathbf{x} \sim p_{\mathrm{data}}}[\frac{1}{2}\left\|\nabla_{\mathbf{x}} \log p_\theta(\mathbf{x})\right\|_2^2+\operatorname{tr}(\underbrace{\nabla_{\mathbf{x}}^2 \log p_\theta(\mathbf{x})}_{\text {Hessian of } \log p_\theta(\mathbf{x})})]+\text { const. }
$$

æ­¤æ—¶æˆ‘ä»¬æƒ³é€šè¿‡å°æ ·æœ¬batchè®­ç»ƒçš„è¯ï¼Œ${x_1, x_2, \dots, x_n} \sim p_{data}(x)$ï¼Œç”¨ç»éªŒå¹³å‡ä¼°è®¡ï¼š

$$
\begin{aligned}
& \frac{1}{n} \sum_{i=1}^n\left[\frac{1}{2}\left\|\nabla_{\mathbf{x}} \log p_\theta\left(\mathbf{x}_i\right)\right\|_2^2+\operatorname{tr}\left(\nabla_{\mathbf{x}}^2 \log p_\theta\left(\mathbf{x}_i\right)\right)\right] \\
= & \frac{1}{n} \sum_{i=1}^n\left[\frac{1}{2}\left\|\nabla_{\mathbf{x}} f_\theta\left(\mathbf{x}_i\right)\right\|_2^2+\operatorname{trace}\left(\nabla_{\mathbf{x}}^2 f_\theta\left(\mathbf{x}_i\right)\right)\right]
\end{aligned}
$$

å†é‡‡ç”¨stochastic gradient descent


- è¿™æ ·æˆ‘ä»¬å°±ä¸ç”¨åœ¨EBMä¸­è¿›è¡Œsampleäº†ï¼
- ä½†æ˜¯æµ·èµ›çŸ©é˜µå¾ˆéš¾ç®—ï¼ï¼ï¼ï¼ˆï¼‰ï¼ˆï¼‰ï¼ˆï¼‰ğŸ¥µ


### Noise contrastive estimation

> TBD




















<script>
    pseudocode.renderClass("pseudocode");
</script>