---
comment: true
---

# Autoregressive Model

!!! abstract
    è‡ªå›å½’æ¨¡å‹æ˜¯ä½¿ç”¨éå¸¸å¹¿æ³›çš„ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨å…ˆå‰çš„ç²—ç•¥å­¦ä¹ ä¸­æˆ‘å¤§æ¦‚çŸ¥é“äº†éƒ¨åˆ†æ¶æ„å¦‚CNNã€RNNã€Attentionç­‰ï¼Œä½†æ˜¯æ²¡æœ‰ç»†è‡´çš„äº†è§£è¿‡ï¼Œè¿™ä¸€éƒ¨åˆ†ä¼šå‚è€ƒMITé‚£ä»½æ•™æå¤šä¸€äº›


## Conditional Distribution Modeling
>æ¡ä»¶åˆ†å¸ƒå»ºæ¨¡å¹¶ä¸åªé€‚ç”¨äºautoregressive model

åœ¨å…ˆå‰çš„å­¦ä¹ ä¸­æˆ‘ä»¬ä½¿ç”¨å¤šä¸ªç‹¬ç«‹éšæœºå˜é‡åˆ†å¸ƒæ¥å»ºæ¨¡ï¼Œä½†æ˜¯åœ¨å®é™…ç”Ÿæ´»ä¸­å…¶å®å¾ˆå¤šåˆ†å¸ƒæ˜¯ç›¸äº’ä¾èµ–çš„

åœ¨å…ˆå‰çš„å­¦ä¹ ä¸­ï¼ˆVAEï¼‰ï¼Œæˆ‘ä»¬æ˜¯åˆ©ç”¨ç‹¬ç«‹çš„æ½œå˜é‡è¿›è¡Œå»ºæ¨¡ï¼Œä½†æ˜¯è¿™éœ€è¦strict assumption for high-dimensional dataï¼ˆeg., 32x32x3 pixelsï¼‰ä½†æ˜¯è¿™å¾ˆéš¾ï¼š

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/ar/ar1.png" style="width: 80%;">
</div>

æ‰€ä»¥æˆ‘ä»¬å¼•å…¥å¦ä¸€ç§æ–¹å¼ï¼šé‡‡ç”¨æ¡ä»¶åˆ†å¸ƒå»ºæ¨¡

Chain Ruleåœ¨è¿™é‡Œè¢«åº”ç”¨ï¼š

ä»»ä½•ä¸€ä¸ªè”åˆåˆ†å¸ƒéƒ½èƒ½è¢«å†™æˆæ¡ä»¶åˆ†å¸ƒçš„ä¹˜ç§¯ï¼š

$$
p(A, B, C) = p(A)p(B|A)p(C|A, B)
$$

å¹¶ä¸”æ˜¯

- ä»»æ„æ’åˆ—çš„ï¼š

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/ar/ar2.png" style="width: 80%;">
</div>

- ä»»æ„åˆ†è§£çš„ï¼š

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/ar/ar3.png" style="width: 80%;">
</div>


!!! example "Case Study"

    === "Case 1"

        <div style="text-align: center;">
            <img src="/../../../../assets/pics/ai/dgm/ar/ar4.png" style="width: 100%;">
        </div>

        è¿™é€‚ç”¨äºæ¯ä¸ªoutputä¾èµ–ä¹‹å‰æ‰€æœ‰çš„outputï¼Œå¦‚language modelæˆ–è€…pixel prediction

    === "Case 2"        

        <div style="text-align: center;">
            <img src="/../../../../assets/pics/ai/dgm/ar/ar5.png" style="width: 100%;">
        </div>

    === "Case 3"

        <div style="text-align: center;">
            <img src="/../../../../assets/pics/ai/dgm/ar/ar6.png" style="width: 100%;">
        </div>

        æ¨¡å‹å°†noisy images($x_T$)è½¬å˜ä¸ºclean images($x_0$)çš„è¿‡ç¨‹æ˜¯diffusion process


åœ¨å¯¹è”åˆåˆ†å¸ƒè¿›è¡Œå»ºæ¨¡æ—¶ï¼šæˆ‘ä»¬ä½¿ç”¨ç¥ç»ç½‘ç»œå¯¹æ¡ä»¶åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡

$$
p(A, B, C) = p_{\theta}(A)p_{\phi}(B|A)p_{\psi}(C|A, B)
$$

- ç†è®ºä¸Šæ¥è¯´æ¯ä¸ªpæ‹¥æœ‰è‡ªå·±çš„weightï¼Œå¦‚æœä½¿ç”¨weight sharingåˆ™æ„å‘³ç€inductive bias


## Autoregression

**Auto**ï¼šself

- using its own outputs for next prediction

**Regression**ï¼š

- estimating relationship between variables

!!! note 

    - è‡ªå›å½’æ˜¯æŒ‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„è¡Œä¸º(inference-time)

    - ä½†æ˜¯è®­ç»ƒæ—¶(training-time)ä¸ä¸€å®šæ˜¯è‡ªå›å½’çš„(eg., teacher-forcingå°±é‡‡ç”¨çœŸå®ç›®æ ‡è¾“å‡ºä½œä¸ºè¾“å…¥ä»¥åŠ é€Ÿè®­ç»ƒï¼Œæ²¡æœ‰è¡¨ç°è‡ªå›å½’ç‰¹æ€§)
    

In Generalï¼š

$$
\begin{align}
p(x_1, x_2, \cdots, x_n) &= p(x_1)p(x_2|x_1)p(x_3|x_1, x_2) \cdots p(x_n|x_1, x_2, \cdots, x_{n-1}) \\
&= \prod_{i=1}^{n} p(x_i|x_1, x_2, \cdots, x_{i-1})
\end{align}
$$


- $x$ can be any representation:

    å¹¶ä¸è¦æ±‚æ˜¯åºåˆ—çš„æˆ–è€…æ—¶é—´ç›¸å…³çš„(sequencial/temporal)

    ä¾‹å¦‚ä»»æ„ç»´æ•°çš„å‘é‡

- $x$ can be any order or partition

    orderï¼š$x_n$ åˆ° $x_1$ ä¹Ÿæ˜¯ä¸€ç§åˆæ³•çš„ç”Ÿæˆé¡ºåº

    partitionï¼šæ¯ä¸ªxéƒ½å¯ä»¥æ˜¯æ ‡é‡ã€å‘é‡ã€å¼ é‡

- $p(\dot|\dot)$ can take any form

    å¯ä»¥æ˜¯å„ç§å½¢å¼çš„æ•°æ®ç»“æ„ï¼Œç¦»æ•£è¿ç»­éƒ½æ˜¯æ”¯æŒçš„

## Inductive Bias

åœ¨ä½¿ç”¨Chain Ruleæ—¶ï¼Œæˆ‘ä»¬çš„å…¬å¼æ˜¯ä¸æ¶‰åŠè¿‘ä¼¼çš„ï¼Œæ˜¯ä¸€ä¸ªå®Œå…¨å‡†ç¡®çš„è¡¨è¾¾å½¢å¼ï¼Œä½†æ˜¯è¿™ä¼šå¯¼è‡´æ¨¡å‹å‚æ•°çˆ†ç‚¸ğŸ’¥ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦è€ƒè™‘åˆ°ä¸€äº›å½’çº³åç½®

é€šå¸¸æ¥è¯´æ¯ä¸ªpéƒ½æ˜¯ä¸åŒçš„æ˜ å°„ï¼Œä½†æˆ‘ä»¬å¼•å…¥shared architecture(RNN, CNN, Transformer)å’Œshared weight $\theta$åå°±å‡ºç°äº†approximation



## Inference of AR

åœ¨åŸå§‹Chain Ruleä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°è¿™æ ·çš„æ¨ç†è¿‡ç¨‹ï¼š

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/ar/ar7.png" style="width: 100%;">
</div>


å¦‚æœæˆ‘ä»¬ç›´æ¥bpè¿™ä¸ªè¿‡ç¨‹ï¼š

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/ar/ar8.png" style="width: 60%;">
</div>

è¿™æ¡è·¯å¾„ä¼šèµ°éæ‰€æœ‰å…ˆå‰æ‰€æœ‰çš„outputsï¼Œæ‰€æœ‰çš„sampling operationå’Œæ‰€æœ‰çš„networks

å› æ­¤æ˜¯infeasibleâŒ to train a AR following its inference graph


## Training

æˆ‘ä»¬é‡‡ç”¨teacher-forcingçš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œå°±æ˜¯ä¸è€ƒè™‘å…ˆå‰çš„è¾“å‡ºç»“æœï¼Œç›´æ¥ä½¿ç”¨ground truth data

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/ar/ar9.png" style="width: 60%;">
</div>


- Prosï¼š

    - bpè·¯å¾„çŸ­å¾—å¤š
    - ground truth inputs can ease training

- Consï¼š

    - inconsistentï¼šåœ¨è®­ç»ƒä¸æ¨ç†æ—¶ä¸ä¸€è‡´ï¼Œå› ä¸ºåœ¨æ¨ç†æ—¶æ²¡æœ‰ground truthæ¥æ ¡æ­£é”™è¯¯
    - distribution shiftï¼šä¸èƒ½çœ‹è§è‡ªå·±çš„é”™è¯¯



## Network Architecture
>Autoregression is **not** architecture-specificâ—ï¸

å…ˆå‰æˆ‘ä»¬è®¨è®ºçš„ä¸œè¥¿éƒ½ä¸æ¶‰åŠå…·ä½“çš„æ¶æ„ï¼Œç°åœ¨æˆ‘ä»¬è®¨è®ºä¸€ä¸‹å…·ä½“çš„æ¶æ„


### Shared Computation

ä¹‹å‰æˆ‘ä»¬è¯´å¯ä»¥ä½¿ç”¨shared archã€weightï¼Œç°åœ¨æˆ‘ä»¬å¯¹ç¥ç»ç½‘ç»œå±‚è¿›è¡Œshare

å¦‚æœå¯¹æ¯ä¸ª$j \geq i$ï¼Œoutput $x_i$ not depend on $x_j$(å°±æ˜¯è¾“å‡ºåªä¾èµ–ä¹‹å‰è€Œä¸ä¾èµ–ä¹‹å)

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/ar/ar10.png" style="width: 60%;">
</div>


### RNN
>Recurrent Neural Network

æ„å»ºä¸€ä¸ªRNNçš„æ­¥éª¤å¯ä»¥è¿™æ ·è¡¨ç¤ºï¼š

=== "One RNN Unit"

    <div style="text-align: center;">
        <img src="/../../../../assets/pics/ai/dgm/ar/ar11.png" style="width: 20%;">
    </div>

=== "Unfold in "time""

    <div style="text-align: center;">
        <img src="/../../../../assets/pics/ai/dgm/ar/ar12.png" style="width: 40%;">
    </div>

=== "Go deep"

    <div style="text-align: center;">
        <img src="/../../../../assets/pics/ai/dgm/ar/ar13.png" style="width: 40%;">
    </div>

RNNå¯ä»¥ç†è§£ä¸ºï¼škeep a summary and recursively update it

åœ¨è¿™é‡Œhidden layer $h_i$å°±æ˜¯é‚£ä¸ªsummary

æˆ‘ä»¬è¿›è¡Œæ›´æ–°çš„è¿‡ç¨‹å°±æ˜¯ï¼š

$$
h_{t+1} = \sigma(W_{hh}h_t + W_{xh}x_{t+1})
$$

ç„¶åè¿›è¡Œé¢„æµ‹ï¼š

$$
o_{t+1} = W_{hy}h_{t+1}
$$

ä½†æ˜¯è¦æ³¨æ„æˆ‘ä»¬çš„summaryæ˜¯éœ€è¦è¿›è¡Œåˆå§‹åŒ–çš„$h_0 = b_0$

æœ€ç»ˆæˆ‘ä»¬å¯¹ä¸‰ä¸ªçŸ©é˜µè¿›è¡Œè®­ç»ƒï¼Œè¿™ä¸‰ä¸ªçŸ©é˜µçš„å‚æ•°å…³äºnæ˜¯å¸¸æ•°çš„

???+ example 
    
    Character RNN(Andrej Karpathy)

    <div style="text-align: center;">
        <img src="/../../../../assets/pics/ai/dgm/ar/ar14.png" style="width: 60%;">
    </div>

    <div style="text-align: center;"    >
        <img src="/../../../../assets/pics/ai/dgm/ar/ar15.png" style="width: 60%;">
    </div>

- Pros:

    - å¯¹ä»»æ„é•¿åº¦éƒ½é€‚ç”¨
    - é€šç”¨æ€§ï¼šå¯¹ä»»æ„computable functionéƒ½å­˜åœ¨ä¸€ä¸ªfinite RNN


- Cons:

    - éœ€è¦æ’åº
    - Sequencial likelihood evaluation è®­ç»ƒæ—¶å¾ˆæ…¢ï¼ˆå› ä¸ºæ— æ³•å¹¶è¡Œè®¡ç®—ï¼‰
    - ç”Ÿæˆæ—¶æ˜¯åºåˆ—ç”Ÿæˆ
    - å¯¹é•¿åºåˆ—ä¼šæœ‰æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜





### CNN
>Convolutional Neural Network

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/ar/ar16.png" style="width: 60%;">
</div>

å·ç§¯ç¥ç»ç½‘ç»œä¾èµ–çš„æ˜¯æ¯ä¸ªâ€œåƒç´ â€çš„å±€éƒ¨æ€§ï¼Œä¹Ÿå°±æ˜¯ä»–ä»¬ä¸å‘¨è¾¹ç©ºé—´çš„ç›¸äº’ä¾èµ–å…³ç³»ï¼Œæˆ‘çš„ç†è§£æ˜¯å·ç§¯æ“ä½œå°±æ˜¯å¯¹åŸå§‹æ•°æ®è¿›è¡Œdown samplingçš„è¿‡ç¨‹ï¼Œè®©æ•°æ®ä¸­è•´å«çš„ä¾èµ–å…³ç³»æ›´åŠ æ˜æ˜¾ï¼Œè¿™å…¶ä¸­é‡‡ç”¨çš„å„ç±»æ“ä½œå¦‚å·ç§¯ã€æ± åŒ–ã€æ¿€æ´»å‡½æ•°ç­‰éƒ½æ˜¯ä¸ºäº†è¿™ä¸ªç›®çš„ã€‚


åœ¨ARä¸­ï¼Œæˆ‘ä»¬è¿™æ ·æ„å»º
<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/ar/ar17.png" style="width: 30%;">
</div>

### Attention








