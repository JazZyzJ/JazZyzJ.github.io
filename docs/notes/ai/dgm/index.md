---
comment: true
---





# Deep Generative Models

!!! abstract
    ç”Ÿæˆå¼æ¨¡å‹è¢«å¹¿æ³›åº”ç”¨äºè§†é¢‘å›¾åƒç”Ÿæˆã€æ–‡æœ¬ç”Ÿæˆã€è¯­éŸ³ç”Ÿæˆç­‰é¢†åŸŸï¼Œæˆ‘åœ¨25å¹´åˆï¼ˆå¯’å‡ï¼‰å¼€å§‹è¿›è¡Œè¿™éƒ¨åˆ†çš„å­¦ä¹ ï¼Œä¸»è¦å‚è€ƒçš„æ˜¯[Stanford CS236](https://deepgenerativemodels.github.io/syllabus.html) è¯¾ç¨‹å’Œ[MIT 6.S987](https://mit-6s978.github.io/schedule.html) è¯¾ç¨‹çš„è®²ä¹‰ï¼Œè¿˜æœ‰[æˆ‘å“¥çš„ç¬”è®°](https://zhuanlan.zhihu.com/p/631001372)ï¼Œ[Lilian Wengçš„åšå®¢](https://lilianweng.github.io/posts/)ã€[Yang Songçš„åšå®¢](https://yang-song.net/blog)

    è¿›è¡Œè¿™éƒ¨åˆ†å­¦ä¹ çš„åˆè¡·æ˜¯åœ¨[ä¾¯è€å¸ˆçš„å®éªŒå®¤](http://cadd.zju.edu.cn/)ï¼Œæˆ‘åœ¨[ç§¦ç¿](https://sorui-qin.github.io/)å¸ˆå…„çš„æŒ‡å¯¼ä¸‹å¼€å±•å…³äºåˆ†å­ç”Ÿæˆå·¥ä½œçš„è¯„æµ‹ï¼Œç”±äºä¸æƒ³å±€é™äºä½¿ç”¨ç°æœ‰æ¨¡å‹è€Œæ˜¯èƒ½æ›´æ·±å…¥çš„ç†è§£æ¨¡å‹çš„å·¥ä½œåŸç†ï¼Œå› æ­¤å¼€å§‹è¿›è¡Œè¿™éƒ¨åˆ†çš„å­¦ä¹ ã€‚
    
    - [x] [VAE](./vae.md)
    - [x] [Autoregressive Models](./ar.md)
    - [x] [Normalizing Flows](./nf.md)
    - [ ] [Ganerative Adversarial Networks](./gan.md)
    - [x] [Energy-based Models](./energy.md)
    - [x] [Score-based Models](./score.md)

    åœ¨Diffusionå’ŒFlowMatchingçš„å­¦ä¹ ä¸­ï¼Œæˆ‘ç»“åˆäº†MITçš„[6.S184](https://diffusion.csail.mit.edu)è¯¾ç¨‹ï¼Œè¿™é—¨è¯¾æä¾›äº†è§†é¢‘è®²è§£+è¯¦ç»†ç¬”è®°+ä»£ç ä½œä¸šï¼ˆèµğŸ‘ï¼Œæ›´é‡è¦çš„æ˜¯ä»–ä»Applicationè§’åº¦è¿›è¡Œäº†éƒ¨åˆ†æˆè¯¾ï¼ŒåŒ…æ‹¬çƒ­é—¨çš„AIGCä»¥åŠæˆ‘å°†æ¥æƒ³å°è¯•çš„Protein Designï¼ï¼ˆè¿™ä¸¤éƒ¨åˆ†æˆ‘å¯èƒ½ä¼šå¤šä¸€äº›ä»£ç çš„å­¦ä¹ å°è¯•ğŸ¤ª

    - [x] [Diffusion Models](./diffusion.md)
    - [x] [Flow Matching](./fm.md)
    - [x] [Discrete Diffusion Models](./ddm.md)

    åŒæ—¶Stanford CS236è¿˜æœ‰ä¸€èŠ‚è¯¾ä¸“é—¨è®²äº†[Evaluating Generative Models](#evaluating-generative-models)ï¼Œè¿™æ˜¯æˆ‘è®¤ä¸ºåœ¨GenAIå‘å±•å¦‚æ­¤è¿…é€Ÿçš„æ—¶ä»£ä¸€ä»¶æœ€æœ‰æ„ä¹‰çš„ä¸€ä»¶äº‹ï¼ˆä¹‹ä¸€ï¼‰:
    
    **how to evaluate your model?**

## Intro

æˆ‘ä»¬è¯´ç”Ÿæˆå¼æ¨¡å‹ï¼ˆGenerative Modelï¼‰ï¼Œä¸ä¹‹ç›¸å¯¹çš„æ˜¯åˆ¤åˆ«å¼æ¨¡å‹ï¼ˆDiscriminative Modelï¼‰ï¼š

- Discriminativeï¼š

    sample $x \rightarrow$ label $y$

    åªæœ‰ä¸€ä¸ªæœŸæœ›è¾“å‡º

- Generativeï¼š

    label $y \rightarrow$ sample $x$

    æœ‰å¤šä¸ªæœŸæœ›è¾“å‡º

???+ example "x and y"

    - åœ¨chatbotä¸­ï¼Œ$y$æ˜¯promptï¼Œ$x$æ˜¯response
    - åœ¨è›‹ç™½è´¨ç”Ÿæˆä¸­ï¼Œ$y$æ˜¯æ€§è´¨/çº¦æŸï¼Œ$x$æ˜¯è›‹ç™½è´¨ç»“æ„


![](../../assets/pics/ai/dgm/dgm/dgm1.png)

å¹¶ä¸”åˆ©ç”¨è´å¶æ–¯å…¬å¼æˆ‘ä»¬å¯ä»¥ç”±ç”Ÿæˆå¼æ¨¡å‹çš„æ¦‚ç‡å¾—åˆ°åˆ¤åˆ«å¼ï¼š

$$
p(y|x) = \frac{p(x|y)p(y)}{p(x)}
$$

ä½†æ˜¯åä¹‹å¹¶ä¸èƒ½å¾ˆå¥½çš„æˆç«‹ï¼š

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/dgm/dgm2.png" style="width: 80%;">
</div>

å› ä¸ºæˆ‘ä»¬éœ€è¦çŸ¥é“æ•°æ®sampleçš„åˆ†å¸ƒï¼Œä½†æ˜¯åªæœ‰ç”Ÿæˆå¼æ¦‚ç‡$p(x|y)$ï¼Œè€Œ$p(x)$æ˜¯ä¸çŸ¥é“çš„ã€‚

!!! summary "æ€»ç»“"

    ç”Ÿæˆå¼æ¨¡å‹å°±æ˜¯è¦å»å¯»æ‰¾æ•°æ®çš„æ½œåœ¨åˆ†å¸ƒ$p(x)$ï¼Œæ¥ç”Ÿæˆä¸çœŸå®æ•°æ®åˆ†å¸ƒç›¸ä¼¼çš„æ ·æœ¬

## Probabilistic Modeling

å‰æ–‡ä¸­æˆ‘ä»¬æåˆ°äº†ä¸€ç³»åˆ—æ¦‚ç‡$p$ï¼Œä½†æ˜¯è¿™äº›æ¦‚ç‡æ˜¯å“ªé‡Œæ¥çš„å‘¢ï¼Ÿ

!!! Tip "Hint"
    <div style="text-align: center;">
        <span style="font-size: 1.5em;">
            Probability is part of the modeling.
        </span>
    </div>

æ€ä¹ˆç†è§£è¿™å¥è¯å‘¢ï¼Œæˆ‘çš„ç†è§£å°±æ˜¯æˆ‘ä»¬åœ¨å­¦ä¹ æ—¶ï¼Œå…¶å®å°±æ˜¯åœ¨å­¦ä¹ ä¸€ç§æ¦‚ç‡åˆ†å¸ƒï¼Œå¯¹è§‚æµ‹çš„æ•°æ®è¿›è¡Œå»ºæ¨¡ï¼Œæ‰€ä»¥å…¶å®å¾—åˆ°çš„åˆ†å¸ƒå‡½æ•°å°±æ˜¯æˆ‘ä»¬çš„æ¨¡å‹ã€‚

è¿™æ ·è¯´æœ‰ç‚¹æŠ½è±¡ï¼Œè¯•ç€ä¸¾ä¸ªä¾‹å­ï¼š

æˆ‘ä»¬ä»¥ç”Ÿæˆå¼æ¨¡å‹ä¸ºä¾‹ï¼Œé‡‡ç”¨æ¦‚ç‡å»ºæ¨¡çš„æ–¹æ³•ï¼š

???+ example "Image Generation"

    - æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡ç»™å®šçš„ä¸€äº›å›¾åƒï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„å›¾åƒ

    ![](../../assets/pics/ai/dgm/dgm/dgm3.png)
    
    - åœ¨é€šè¿‡ä¸€ç³»åˆ—çš„æ–¹æ³•ï¼Œå¾—åˆ°ä¸€ä¸ªä¼°è®¡ï¼ˆestimatedï¼‰çš„åˆ†å¸ƒï¼Œè¿™ä¸ªåˆ†å¸ƒçš„è¯„ä¼°æ˜¯é€šè¿‡æŸå¤±å‡½æ•°$L$æ¥è¿›è¡Œçš„
  
    ![](../../assets/pics/ai/dgm/dgm/dgm4.png)

    - æ­¤æ—¶æˆ‘ä»¬ä¾ç…§ç»™å®šç‰¹å¾$y$ï¼Œç”Ÿæˆä¸€ä¸ªå›¾åƒ$x'$ï¼Œè¿™ä¸ª$x'$å°±éµä»æˆ‘ä»¬å¾—åˆ°çš„åˆ†å¸ƒ$p(x|y)$

    ![](../../assets/pics/ai/dgm/dgm/dgm5.png)

!!! note "Notes"
    
    - Generative models involve statistical models which are often designed and derived by humans.
    - Probabilistic modeling is not just work of neural nets.
    - Probabilistic modeling is a popular way, but not the only way.




## "Deep" Generative Models

æ·±åº¦å­¦ä¹ æ˜¯ä¸€ç§è¡¨å¾å­¦ä¹ ï¼Œä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬å­¦ä¹ çš„æ˜¯å¦‚ä½•å°†æ•°æ® $x$ æ˜ å°„åˆ°$f(x)$ï¼Œä½¿å¾—æŸå¤±å‡½æ•°$L(f(x), y)$æœ€å°ã€‚

åœ¨æ·±åº¦ç”Ÿæˆæ¨¡å‹çš„å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ çš„æ˜¯å¦‚ä½•è¡¨å¾æ¦‚ç‡åˆ†å¸ƒ

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/dgm/dgm6.png" style="width: 80%;">
</div>

è¿™é‡Œæˆ‘ä»¬å­¦ä¹ å¾—åˆ°ä¸€ä¸ªç®€å•åˆ†å¸ƒåˆ°å¤æ‚åˆ†å¸ƒçš„æ˜ å°„ï¼Œè¿™ä¸ªæ˜ å°„å°±æ˜¯æˆ‘ä»¬çš„æ¨¡å‹ã€‚

$$
 \pi \rightarrow g(\pi)
$$

åƒè¿™æ ·ï¼š

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/dgm/dgm7.png" style="width: 80%;">
</div>

éšåæœ€å°åŒ–åŸºäºæ•°æ®çš„æŸå¤±å‡½æ•°$L(p_x , g(\pi))$ï¼Œå¾—åˆ°æ¨¡å‹$g$

!!! summary "æ€»ç»“"

    ä¸€ä¸ªDGMå¯èƒ½åŒ…æ‹¬ï¼š

    - Formulationï¼š
        - formulate a problem as a probabilistic modeling ï¼ˆè¿›è¡Œæ¦‚ç‡å»ºæ¨¡ï¼‰
        - decompose a complex distribution into simpler and tractable ones ï¼ˆå°†å¤æ‚åˆ†å¸ƒåˆ†è§£ï¼‰
    - Representationï¼šdeep neural networks to represent data and their distributions ï¼ˆä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¡¨ç¤ºæ•°æ®å’Œä»–ä»¬çš„åˆ†å¸ƒï¼‰
    - Objective functionï¼ševaluate the predicted distribution
    - Optimizationï¼šoptimize the networks and/or the decomposition
    - Inference:
        - sampler 
        - probability density estimator
        - ...

## Latent Variable Models

åœ¨è¿™é‡Œæˆ‘è§‰å¾—éœ€è¦æå‰è¯´æ˜çš„æ˜¯ï¼Œåœ¨æ·±åº¦ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä½¿ç”¨éšå˜é‡ï¼ˆLatent Variablesï¼‰

ä¾‹å¦‚è¿™æ ·ä¸€ç»„å›¾ç‰‡ï¼š

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/dgm/dgm8.png" style="width: 50%;">
</div>

ç”±äºæ€§åˆ«ã€å¹´é¾„ã€è‚¤è‰²ç­‰ç­‰å› ç´ ï¼Œå›¾åƒ$x$å­˜åœ¨å¤šç§å¯èƒ½çš„å˜åŒ–ï¼Œä½†é™¤éå›¾ç‰‡æ˜¯è¢«æ³¨é‡Šannotatedçš„ï¼Œå¦åˆ™æˆ‘ä»¬æ— æ³•å¾—çŸ¥è¿™äº›å› ç´ ï¼Œæˆ–è€…è¯´ï¼Œè¿™äº›å› ç´ æ˜¯ä¸å¯è§çš„ï¼ˆnot explicitly availableï¼‰ã€‚

!!! tip "Latent Variable"
    è¿™æ—¶æˆ‘ä»¬çš„æƒ³æ³•å°±æ˜¯ç”¨éšå˜é‡$z$æ˜¾ç¤ºçš„å»ºæ¨¡æ¥è¡¨ç¤ºè¿™äº›å› ç´ ã€‚


å¾ˆç›´è§‚çš„æƒ³æ³•æ˜¯ç”¨ Bayes ç½‘ç»œæ¥è¡¨è¾¾ï¼š

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/dgm/dgm9.png" style="width: 80%;">
</div>

ä½†æ˜¯è¿™å…¶ä¸­çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒæ˜¯å¾ˆéš¾å¾—åˆ°çš„ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼è¿™ä¸ªæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼š

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/dgm/dgm10.png" style="width: 80%;">
</div>  

è¿™æ—¶æˆ‘ä»¬é€šå¸¸å‡è®¾$z$æ˜¯æœä»æŸç§ç®€å•åˆ†å¸ƒçš„ï¼Œä¾‹å¦‚é«˜æ–¯åˆ†å¸ƒï¼š$z \sim N(0, 1)$

é€šè¿‡ç¥ç»ç½‘ç»œçš„å¤„ç†ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°$p(x|z)=N(x|\mu_{\theta}(z), \sigma_{\theta}(z))$ï¼Œåœ¨è¿™é‡Œ$\theta$æ˜¯ç¥ç»ç½‘ç»œçš„å‚æ•°ã€‚

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/dgm/dgm11.png" style="width: 60%;">
</div>

æˆ‘ä»¬å¸Œæœ›åœ¨è®­ç»ƒç»“æŸåï¼Œ$z$å¯ä»¥è¡¨ç¤º$x$çš„æ½œåœ¨å› ç´ ï¼ˆç‰¹å¾ï¼‰

ä½¿ç”¨$z$æ¥è¡¨ç¤ºæœ‰ä¸¤ç‚¹åŸå› ï¼š

1. ç”¨æ½œå˜é‡å¯ä»¥ç®€åŒ–é—®é¢˜
2. å¾—åˆ°çš„æ½œå˜é‡æœ¬èº«å°±å…·æœ‰ä»–çš„æ„ä¹‰ï¼ˆä¸ç”¨åœ¨ç”Ÿæˆï¼Œç”¨åœ¨æ¨æ–­æˆ–å¯»æ‰¾ç‰¹å¾ï¼‰


## Measure Distribution

åœ¨å¾—åˆ°ç”Ÿæˆåˆ†å¸ƒåï¼Œæˆ‘ä»¬éœ€è¦è¯„ä¼°è¿™ä¸ªåˆ†å¸ƒçš„ä¼˜åŠ£ï¼Œè¿™é‡Œå¯¹å‡ ä¸ªå¸¸ç”¨çš„è¯„ä¼°æ–¹æ³•è¿›è¡Œä»‹ç»ã€‚

###  Divergence

ç»™å®šä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒ$P$å’Œ$Q$ï¼Œ$f$-divergenceå®šä¹‰ä¸ºï¼š

$$
D_{f}(P||Q) = \int {q(x)} f\left(\frac{p(x)}{q(x)}\right) dx
$$


- KL



KL æ•£åº¦ï¼ˆKullback-Leibler divergenceï¼‰: å¯¹äºç»™å®šçš„ä¸¤ä¸ªåˆ†å¸ƒ$p$å’Œ$q$ï¼ŒKLæ•£åº¦å®šä¹‰ä¸ºï¼š

ç¦»æ•£çš„ï¼š

$$
D_{KL}(p||q) = \sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)}
$$

è¿ç»­çš„ï¼š

$$
D_{KL}(p||q) = \int p(x) \log \frac{p(x)}{q(x)} dx
$$


å¯ä»¥çœ‹åˆ°KLæ•£åº¦æ˜¯$f$-divergenceçš„ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼Œå½“$f(x) = x \log x$æ—¶ï¼Œ$f$-divergenceå°±æ˜¯KLæ•£åº¦ã€‚

- éè´Ÿæ€§ï¼š$D_{KL}(p||q) \geq 0$

- KLæ•£åº¦è¶Šå°ï¼Œä¸¤ä¸ªåˆ†å¸ƒè¶Šç›¸ä¼¼ã€‚

- KLæ•£åº¦ä¸å…·æœ‰å¯¹ç§°æ€§ï¼Œå³$D_{KL}(p||q) \neq D_{KL}(q||p)$



åœ¨å¯¹$p_{\theta}$å’Œ$p_{data}$è¿›è¡Œæ¯”è¾ƒæ—¶ï¼Œæˆ‘ä»¬å–è¿™ä¸¤ä¸ªå˜é‡çš„KLæ•£åº¦æœ€å°å€¼ï¼š

$$
\min_{\theta} D_{KL}(p_{data}||p_{\theta})
$$

å¯ä»¥æ¨å¯¼KLæ•£åº¦æœ€å°å€¼ç­‰ä»·äºæœ€å¤§åŒ– ==æœŸæœ›å¯¹æ•°ä¼¼ç„¶==ï¼ˆMaximum Log-Likelihood Estimationï¼‰ï¼šï¼ˆç”¨ç¦»æ•£çš„KLå…¬å¼ï¼Œæ‰“å¼€$\log$ï¼Œå…¶ä¸­$p_{data}(x)$æ˜¯å¸¸æ•°ï¼Œæ‰€ä»¥å¯ä»¥å¿½ç•¥ï¼‰

$$
arg\min_{\theta} D_{KL}(p_{data}||p_{\theta}) = arg\max_{\theta} \mathbb{E}_{x \sim p_{data}} \log p_{\theta}(x)
$$

!!! warning "ç¼ºé™·"

    - ç”±äºæˆ‘ä»¬å¿½ç•¥äº†$p_{data}(x)$çš„æœŸæœ›é¡¹ï¼Œæ‰€ä»¥æœ€ç»ˆæˆ‘ä»¬åªèƒ½å¾—åˆ°ä¸€ä¸ªå‚æ•°$\theta$çš„å–å€¼çš„ä¼°è®¡ï¼Œä½†æ˜¯ä¸èƒ½çŸ¥é“how close the model is to the true distributionã€‚
    - In practice, we can't compute the true distribution $p_{data}$


å¯¹äºç¬¬äºŒä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¸ºäº†é¿å…æ¶‰åŠåˆ°$p_{data}$ï¼Œæˆ‘ä»¬ä½¿ç”¨å¦ä¸€ç§ä¼¼ç„¶æ–¹æ³•ï¼š==ç»éªŒå¯¹æ•°ä¼¼ç„¶==ï¼ˆEmpirical Log-Likelihoodï¼‰

æœŸæœ›å¯¹æ•°ä¼¼ç„¶æ˜¯å¯¹æ‰€æœ‰æ•°æ®ç‚¹çš„å¯¹æ•°ä¼¼ç„¶æ±‚æœŸæœ›ï¼Œè€Œç»éªŒå¯¹æ•°ä¼¼ç„¶æ˜¯å¯¹æ‰€æœ‰æ•°æ®ç‚¹çš„å¯¹æ•°ä¼¼ç„¶æ±‚å¹³å‡ã€‚

$$
\max_{\theta} \mathbb{E}_{x \sim p_{data}} \log p_{\theta}(x) \approx \max_{p_{\theta}} \frac{1}{N} \sum_{i=1}^{N} \log p_{\theta}(x_i)
$$

è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†æ‰€éœ€è¦çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ã€‚

å¯¹äºæ•°æ®çš„æœ€å¤§åŒ–ä¼¼ç„¶ï¼šï¼ˆè”åˆæ¦‚ç‡ä¼¼ç„¶ï¼‰

$$
p_{\theta}(x^{1}, x^{2}, ..., x^{N}) = \prod_{i=1}^{N} p_{\theta}(x^{i})
$$

---


- Fisher



## Evaluating Generative Models


!!! tip "Key"

    <div style="text-align: center;">
        <span style="font-size: 1.5em;">
            What's the task that you **really care about**?
        </span>
    </div>


æ€»çš„æ¥è¯´æˆ‘ä»¬æœ‰ä»¥ä¸‹å‡ ä¸ªä»»åŠ¡ï¼š

!!! summary "Summary"

    - Density Estimation
    - Compression 
    - Sampling/Generation
    - Latent representation learning
    - Composition tasks


### Density Estimation or Compression

å¯¹likelihoodè¿›è¡Œå»ºæ¨¡ï¼Œå°±æ˜¯å°†æ•°æ®åˆ†å‰²ä¸ºtrainã€valã€testï¼Œç”¨trainè¿›è¡Œè®­ç»ƒçš„åˆ°$p_{\theta}$ï¼Œç„¶åç”¨valè¿›è¡Œtuningï¼Œæœ€åç”¨testè¿›è¡Œè¯„ä¼°ï¼š$E_{p_{data}}[log p_{\theta}(x)]$ï¼Œè¿™é‡Œçš„$p_{data}$æ˜¯çœŸå®çš„æ•°æ®åˆ†å¸ƒï¼Œè¿™ä¸ªæœŸæœ›ç”¨äºè¡¡é‡$p_{\theta}$å¯¹çœŸå®æ•°æ®çš„æ‹Ÿåˆç¨‹åº¦ï¼Œå½“$p_{\theta}$å’Œ$p_{data}$è¶Šæ¥è¿‘ï¼Œè¿™ä¸ªæœŸæœ›è¶Šå¤§ï¼Œä¸¤è€…ç›¸ç­‰æ—¶ï¼Œè¿™ä¸ªæœŸæœ›å°±æ˜¯çœŸå®æ•°æ®çš„**ç†µ**ã€‚

è¿™é‡Œå°±å¼•å…¥äº†**ç†µ**çš„æ¦‚å¿µï¼Œä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬æœ‰**å‹ç¼©å³è¯„ä¼°**çš„è¯´æ³•

åœ¨å‹ç¼©ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›å‡ºç°é¢‘ç‡è¶Šé«˜çš„æ•°æ®ï¼Œå‹ç¼©åçš„é•¿åº¦è¶ŠçŸ­ï¼Œè€Œå‡ºç°é¢‘ç‡è¶Šä½çš„æ•°æ®ï¼Œå‹ç¼©åçš„é•¿åº¦è¶Šé•¿ï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°å‹ç¼©åçš„å¹³å‡é•¿åº¦ï¼Œè¿™ä¸ªå¹³å‡ç é•¿æ­£æ˜¯ä¸Šæ–‡ä¸­æœŸæœ›çš„å€¼ï¼ˆå†…éƒ¨æ˜¯è´Ÿå¯¹æ•°ï¼‰ï¼Œè¿™ä¸ªå€¼è¶Šå°ï¼Œè¯´æ˜æ¨¡å‹è¶Šå¥½ã€‚

ä¸€ä¸ªä¾‹å­å°±æ˜¯å¯¹è¯­è¨€æ¨¡å‹æ¥è¯´ï¼š

$$
\text{Perplexity} =  2^{-\frac{1}{D}E_{p_{data}}[\log p_{\theta}(x)]} \text{  for } x \in \mathbb{R}^D
$$

!!! note 

    è¿™é‡Œæˆ‘çš„ç†è§£æ˜¯ï¼ŒCompressionå¯¹äºæ¨¡å‹æ¥è¯´ï¼Œå°±æ˜¯å­¦ä¹ patternï¼Œå¾—åˆ°patternåçš„æ¨¡å‹ï¼Œå¯ä»¥æŠ›å¼ƒç»™å®šæ•°æ®ä¸‹patternçš„è¾“å‡ºï¼Œç›´æ¥è¾“å‡ºpatternï¼Œè¿™æ ·å°±å»é™¤äº†redundancyã€‚

ä½†é—®é¢˜æ˜¯å¯¹äºå¾ˆå¤šæ¨¡å‹ï¼Œæˆ‘ä»¬æ˜¯æ²¡æœ‰tractable likelihoodè¾“å‡ºçš„ï¼Œä¾‹å¦‚GANï¼ŒVAEï¼ŒEBMsï¼Œé‚£æˆ‘ä»¬æ€ä¹ˆä¼°è®¡likelihoodå¦‚æœæˆ‘ä»¬åªæœ‰samplesï¼Ÿ

æ€»çš„æ¥è¯´ï¼š

<center>

**Unbiased estimation of $p(x)$ from samples is impossible**

</center>

æ‰€ä»¥æˆ‘ä»¬è¦é‡‡ç”¨ä¸€äº›è¿‘ä¼¼çš„æ–¹æ³•ï¼Œä¾‹å¦‚ï¼š


!!! definition "Kernel Density Estimation"

    ç»™å®šä¸€ä¸ªSampleæ ·æœ¬é›†åˆ$S=\{x_1, x_2, ..., x_n\}$ï¼Œæˆ‘ä»¬å¸Œæœ›ä¼°è®¡ä¸€ä¸ªä¸åœ¨$S$ä¸­çš„æ ·æœ¬$x_t$çš„æ¦‚ç‡$p(x_t)$ï¼Œæœ‰å¦‚ä¸‹å…¬å¼ï¼š

    $$
    \hat{p}(x_t) = \frac{1}{n} \sum_{i=1}^{n} K(\frac{x_t-x_i}{\sigma})
    $$

    å…¶ä¸­$K$æ˜¯æ ¸å‡½æ•°ï¼Œ$\sigma$æ˜¯å¸¦å®½å‚æ•°bandwidth parameter

    - K éœ€è¦æ»¡è¶³ä»¥ä¸‹ä¸¤æ¡æ€§è´¨ï¼š
        - å½’ä¸€åŒ–ï¼š$\int K(x) dx = 1$
        - å¯¹ç§°æ€§ï¼š$K(x) = K(-x)$
    
    - Bandwidth parameter $\sigma$ éœ€è¦é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„å€¼(black)ï¼Œå¦‚æœ$\sigma$å¤ªå°ï¼Œåˆ™ä¼°è®¡çš„å¯†åº¦å‡½æ•°ä¼šè¿‡äºå°–é”(red)ï¼Œå¦‚æœ$\sigma$å¤ªå¤§(green)ï¼Œåˆ™ä¼°è®¡çš„å¯†åº¦å‡½æ•°ä¼šè¿‡äºå¹³æ»‘ã€‚

    <div style="text-align: center;">
        <img src="/../../../../assets/pics/ai/dgm/dgm/dgm12.png" style="width: 40%;">
    </div>

æ€»çš„æ¥è¯´æ ¸å¯†åº¦ä¼°è®¡æ˜¯ä¸€ç§æ’å€¼æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯”å¾…ä¼°è®¡ç‚¹ä¸å·²çŸ¥samplesä¹‹é—´çš„è·ç¦»ï¼Œæ’å€¼å–å€¼åœ¨ç”¨å·²çŸ¥samplesç”¨æ ¸å‡½æ•°è¿›è¡Œæ‹Ÿåˆçš„å‡½æ•°æ›²çº¿ä¸Šã€‚

ä½†æ˜¯KDEçš„è®¡ç®—å¤æ‚åº¦æ˜¯$O(n^2)$ï¼Œå…¶åœ¨é«˜ç»´åº¦ä¸Šä¸å¯é 


### Quantitative Evaluations

#### Inception Score

Inception Score ç”¨äºè¯„ä¼°labelled dataç”Ÿæˆè´¨é‡ï¼Œä¸»è¦è€ƒè™‘çš„æ˜¯ç”Ÿæˆæ ·æœ¬çš„sharpnesså’Œdiversity

!!! definition "Inception Score"

    åœ¨ISä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸¤ç‚¹å‡è®¾ï¼š

    - æœ‰æ ‡ç­¾çš„ç›‘ç£å­¦ä¹ æ¨¡å‹
    - å…·æœ‰å¥½çš„åˆ†ç±»å™¨$c(y|x)$

    **Sharpness**ï¼š

    $$
    S=\exp \left(E_{\mathbf{x} \sim p}\left[\int c(y \mid \mathbf{x}) \log c(y \mid \mathbf{x}) \mathrm{d} y\right]\right)
    $$

    **Diversity**ï¼š

    $$
    D=\exp \left(-E_{\mathbf{x} \sim p}\left[\int c(y \mid \mathbf{x}) \log c(y) \mathrm{d} y\right]\right)
    $$

    **IS**ï¼š

    $$
    \text{IS} = D \times S
    $$

å¯¹äºè¿™é‡Œæ¸…æ™°åº¦å’Œå¤šæ ·æ€§çš„ç†è§£ï¼š

- æ¸…æ™°åº¦Sï¼šå¦‚æœæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªéå¸¸æ¸…æ™°çš„å›¾åƒï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥éå¸¸å®¹æ˜“è¯†åˆ«å‡ºä¸€ä¸ªå›¾åƒçš„ç±»åˆ«ï¼Œè¿™æ—¶åˆ†ç±»å™¨$c(y|x)$çš„åˆ†å¸ƒåº”è¯¥æ˜¯ä¸€ä¸ªå°–å³°ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ª**ä½ç†µ**çš„åˆ†å¸ƒï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨ä¸Šè¿°çš„Så…¬å¼æ¥æ±‚ç®—åˆ†ç±»å™¨$c(y|x)$çš„ç†µï¼Œå¹¶è¿½æ±‚ä»–çš„æœ€å°åŒ–

- å¤šæ ·æ€§Dï¼šå¦‚æœæˆ‘ä»¬å¾—åˆ°äº†éå¸¸å¤šçš„ç±»åˆ«ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„è¾¹ç¼˜åˆ†å¸ƒ$c(y)$åº”è¯¥æ˜¯æ¥è¿‘ä¸€ä¸ªå‡åŒ€åˆ†å¸ƒçš„å½¢æ€ï¼Œæ„å‘³ç€æ¯ä¸ªæ ·æœ¬å‡ºç°çš„æ¬¡æ•°éƒ½å·®ä¸å¤šï¼Œæ‰€ä»¥$c(y)$çš„ç†µåº”è¯¥æ˜¯ä¸€ä¸ª**é«˜ç†µ**çš„åˆ†å¸ƒï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨ä¸Šè¿°çš„Då…¬å¼æ¥æ±‚ç®—$c(y)$çš„ç†µï¼Œå¹¶è¿½æ±‚ä»–çš„æœ€å¤§åŒ–

å¦‚æœè¿˜è®°å¾—KL Divergenceçš„å®šä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥å°†ISå…¬å¼æ”¹å†™ä¸ºï¼š

$$
\text{IS} = \exp \left(E_{x \sim p}\left[D_{KL}(c(y|x)||c(y))\right]\right)
$$

è¿™æ—¶maximize ISç­‰ä»·äºæœ€å¤§åŒ–Då’ŒSä¹‹é—´çš„å·®å¼‚ï¼Œä¹Ÿå°±æ˜¯æ—¢è¦æ±‚æ¨¡å‹ç”Ÿæˆæ¸…æ™°åº¦é«˜çš„å›¾åƒï¼Œåˆè¦æ±‚æ¨¡å‹ç”Ÿæˆå¤šæ ·æ€§çš„å›¾åƒï¼Œåªæœ‰å½“ä¸¤è€…éƒ½è¾¾åˆ°æ—¶ï¼ŒISæ‰ä¼šè¾¾åˆ°æœ€å¤§å€¼

ä½†æ˜¯ISçš„é—®é¢˜åœ¨äºï¼š

- ä¾èµ–åˆ†ç±»å™¨
- æ— æ³•æ£€æµ‹ç±»å†…å¤šæ ·æ€§

è€Œä¸”ä½ ä¼šå‘ç°ï¼Œå¥½åƒISå¹¶æ²¡æœ‰åœ¨è€ƒè™‘çœŸå®æ•°æ®çš„åˆ†å¸ƒï¼Œè€Œæ˜¯åªè€ƒè™‘äº†ç”Ÿæˆæ•°æ®çš„åˆ†å¸ƒ

#### FrÃ©chet Inception Distance

FIDçš„æ ¸å¿ƒåœ¨äºï¼Œä¸å…¶é—´æ¥åœ°é€šè¿‡åˆ†ç±»æ¦‚ç‡æ¥è¯„ä¼°ï¼Œä¸å¦‚ç›´æ¥æ¯”è¾ƒçœŸå®æ•°æ®å’Œç”Ÿæˆæ•°æ®åœ¨**æ·±åº¦ç‰¹å¾ç©ºé—´**ä¸­çš„ç»Ÿè®¡åˆ†å¸ƒ

!!! definition "FID Procedure"

    1. Feature Extraction

        å¯¹äºæ¯ä¸€ä¸ªçœŸå®æ•°æ®$x_r$å’Œç”Ÿæˆæ•°æ®$x_g$ï¼Œè¾“å…¥è¿›Inception V3æ¨¡å‹æå–ç‰¹å¾ï¼Œå¾—åˆ°ç‰¹å¾å‘é‡ï¼Œè¿™ä¸ªå‘é‡ä¸ä»…ä»…æ˜¯æµ…å±‚çš„åˆ†ç±»ï¼Œè€Œæ˜¯åŒ…å«äº†æ›´æ·±å±‚çš„ç‰¹å¾ï¼Œä¾‹å¦‚çº¹ç†ã€å½¢çŠ¶ç­‰ï¼Œç±»ä¼¼äºä¸€ä¸ªç‰¹å¾æŒ‡çº¹ï¼Œè¿™æ ·æˆ‘ä»¬å¾—åˆ°äº†ä¸¤ç»„ç‰¹å¾å‘é‡$X_r$å’Œ$X_g$

    2. Fitting a Multivalue Gaussian 
        
        åœ¨è¿™é‡ŒFIDåšå‡ºä¸€ä¸ªå‡è®¾ï¼šè¿™ä¸¤å †ç‰¹å¾å‘é‡çš„åˆ†å¸ƒå¯ä»¥ç”¨å¤šå…ƒé«˜æ–¯æ¥è¿‘ä¼¼æè¿°ï¼Œè¿™ä¸¤ä¸ªæ•°æ®é›†åˆ†åˆ«å…·æœ‰ä»–ä»¬è‡ªå·±çš„é«˜æ–¯å±æ€§ï¼š

        - å‡å€¼å‘é‡$\mu_r$å’Œ$\mu_g$
        - åæ–¹å·®çŸ©é˜µ$\Sigma_r$å’Œ$\Sigma_g$

    3. FrÃ©chet Distance
        
        è®¡ç®—ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼š

        $$
        \text{FID} = ||\mu_r - \mu_g||^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
        $$

åœ¨æœ€åçš„å¼å­ä¸­ï¼Œç¬¬ä¸€é¡¹å°±æ˜¯ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒå‡å€¼ä¹‹é—´çš„è·ç¦»ï¼Œç¬¬äºŒé¡¹é€šè¿‡ä¸¤ä¸ªçŸ©é˜µä¹‹é—´çš„traceè¡¡é‡åæ–¹å·®çŸ©é˜µçš„å·®å¼‚æ€§

#### Kernel Inception Distance

åœ¨è®¨è®ºFIDçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå…³é”®çš„å‡è®¾æ˜¯æ•°æ®çš„åˆ†å¸ƒæœä»é«˜æ–¯åˆ†å¸ƒï¼Œè¿™ä¼šå¯¼è‡´ä¸€äº›å±€é™æ€§ã€‚æˆ‘ä»¬ç°åœ¨å°è¯•ç›´æ¥æ¯”è¾ƒä¸¤ä¸ªæ•°æ®åˆ†å¸ƒæœ¬èº«ï¼š


!!! definition "KID"

    ç»“åˆäº†Inceptionç‰¹å¾æå–å’Œæ ¸æ–¹æ³•çš„åˆ†å¸ƒå·®å¼‚åº¦é‡MMDï¼š

    - MMDï¼šMaximum Mean Discrepancyï¼Œé€šè¿‡å¯¹æ¯”ä¸¤ä¸ªåˆ†å¸ƒåœ¨å†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ä¸­çš„å‡å€¼æ¥è¡¡é‡åˆ†å¸ƒå·®å¼‚

    $$
    \operatorname{MMD}(p, q)=\sqrt{\mathbb{E}_{x, x^{\prime} \sim p}\left[K\left(x, x^{\prime}\right)\right]+\mathbb{E}_{y, y^{\prime} \sim q}\left[K\left(y, y^{\prime}\right)\right]-2 \mathbb{E}_{x \sim p, y \sim q}[K(x, y)]}
    $$

    å…¶ä¸­ï¼Œ$K$æ˜¯æ ¸å‡½æ•°ï¼Œ$x, x'$ å’Œ $y, y'$ åˆ†åˆ«æ˜¯ä»åˆ†å¸ƒ$p$å’Œ$q$ä¸­ç‹¬ç«‹é‡‡æ ·çš„æ ·æœ¬

    å¯¹äºæ•´ä¸ªè¡¨è¾¾å¼ï¼Œç¬¬ä¸€äºŒé¡¹ç”¨äºè¡¡é‡ä¸¤ä¸ªæ•°æ®åˆ†å¸ƒ$p$å’Œ$q$çš„è‡ªç›¸ä¼¼æ€§ï¼Œç¬¬ä¸‰é¡¹ç”¨äºè¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„äº¤å‰ç›¸ä¼¼æ€§

    - KID Procedure

        1. ç‰¹å¾æå–ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„Inceptionç½‘ç»œæå–ç‰¹å¾
        2. è®¡ç®—MMDï¼šè®¡ç®—ç”Ÿæˆæ•°æ®$p_g$å’ŒçœŸå®æ•°æ®$p_r$çš„MMD
        3. è®¡ç®—KIDï¼š

        $$
        \operatorname{KID}(p_g, p_r)= \operatorname{MMD}^2(Inception(p_g), Inception(p_r))
        $$

        å¸¸è§çš„æ ¸å‡½æ•°æœ‰é«˜æ–¯æ ¸æˆ–è€…æ˜¯è®¡ç®—é«˜æ•ˆçš„å¤šé¡¹å¼æ ¸$K(x, y) = (x^T y + c)^d$

KIDçš„ä¼˜ç‚¹ï¼š

- é¿å…äº†é«˜æ–¯åˆ†å¸ƒå‡è®¾
- æ— åä¼°è®¡
- é¿å…äº†traceè®¡ç®—
- é€šè¿‡æ ¸å‡½æ•°éšå«é«˜é˜¶ç»Ÿè®¡é‡ï¼Œæœ‰å¯è§£é‡Šæ€§


>å¯¹äºæœ‰æ— åä¼°è®¡æˆ‘ä¸æ˜¯å¤ªæ¸…æ¥šï¼Œè¿™é‡Œæ”¾ä¸€äº›è§£é‡Šï¼š

??? note "Bias vs. Unbiased"
    
    æœ€ç›´è§‚çš„ä¸€ä¸ªè§£é‡Šæ˜¯ï¼šæ— åä¼°è®¡æ˜¯ä¸€ä¸ªå¼“ç®­æ‰‹èƒ½å¤Ÿå›´ç»•é¶å¿ƒè¿›è¡Œå°„å‡»ï¼Œè€Œåå·®ä¼°è®¡æ˜¯å¼“ç®­æ‰‹å¯ä»¥å›ºå®šä¸€ä¸ªå°„å‡»èŒƒå›´ï¼Œä½†è¿™ä¸ªèŒƒå›´çš„ä¸­å¿ƒä¸æ˜¯é¶å¿ƒï¼ˆç³»ç»Ÿæ€§åç¦»çœŸå®å€¼ï¼‰

    å¯¹äºä¸€ä¸ªçœŸå®å‚æ•°$\theta$ï¼Œæˆ‘ä»¬æœ‰ä¼°è®¡é‡$\hat{\theta}$ï¼Œæˆ‘ä»¬å®šä¹‰æ— åä¼°è®¡ä¸ºï¼š

    $$
    \mathbb{E}[\hat{\theta}] = \theta
    $$
    
    è€Œåå·®ä¼°è®¡ä¸ºï¼š
    
    $$
    \mathbb{E}[\hat{\theta}] \neq \theta
    $$

    åŒæ—¶åå·®é‡å®šä¹‰ä¸ºï¼š

    $$
    \text{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta
    $$

    å¯¹äºå®é™…ä»»åŠ¡æ¥è¯´ï¼šåœ¨å°æ ·æœ¬ä¸‹æœ‰åä¼°è®¡ä¼šç³»ç»Ÿæ€§ä½ä¼°çœŸå®åˆ†å¸ƒå·®å¼‚ï¼ˆåå·®éšæ ·æœ¬é‡å¢å¤§è€Œå‡å°ï¼‰ï¼Œè€Œæ— åä¼°è®¡åˆ™ä¸ä¼šå› ä¸ºæ ·æœ¬é‡çš„å˜åŒ–è€Œå˜åŒ–ï¼Œä½†åŒæ—¶åå·®çš„æœ‰æ— å¹¶ä¸æ˜¯ç»å¯¹çš„å¥½/åï¼Œè€Œæ˜¯å–å†³äºä»»åŠ¡æœ¬èº«ï¼Œå› ä¸ºæœ‰çš„æ—¶å€™æ¨¡å‹ä¼šéœ€è¦é™ä½æ–¹å·®ä¸å¾—ä¸ç‰ºç‰²æ— åæ€§

    
### Latent Representations

è¿™æ˜¯æˆ‘è®¤ä¸ºçš„ä¸€ä¸ªå¾ˆå›°éš¾çš„ä½†æ˜¯å¾ˆæ ¸å¿ƒçš„é—®é¢˜ï¼Œå¹¶ä¸”è¿™ä¸ªé—®é¢˜æ˜¯æ²¡æœ‰é€šè§£çš„ã€‚

æœ‰å‡ ä¸ªæ ¸å¿ƒæˆ–è€…è¯´å¸¸ç”¨çš„æ‰‹æ®µï¼š

- Clustering
- Compression
- Disentanglement

!!! question "Disentanglement"

    è§£è€¦ï¼Œæˆ‘çš„ç†è§£å°±æ˜¯å°†å¤æ‚å¤šå…ƒç³»ç»Ÿæ‹†è§£æˆå•ä¸€å˜é‡æ§åˆ¶çš„ç³»ç»Ÿã€‚
    
    ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬åœ¨æ½œç©ºé—´ä¸­æ‰¾åˆ°ä¸€ä¸ªç‰¹å®šçš„æ½œå˜é‡ï¼Œè¿™ä¸ªæ½œå˜é‡å…·æœ‰æŸç§æ„ä¹‰èƒ½å¤Ÿæ§åˆ¶ç”Ÿæˆæ•°æ®çš„æŸä¸ªç‰¹å¾ï¼Œè¿™æ ·æˆ‘ä»¬é€šè¿‡æ§åˆ¶è¿™ä¸ªå˜é‡å°±èƒ½è¾¾åˆ°å…¨å±€ç‰¹å¾çš„ä¿®æ”¹




    
    






    
    
    
    
    




    

    


