---
comment: true
---





# Deep Generative Models

!!! abstract
    ç”Ÿæˆå¼æ¨¡å‹è¢«å¹¿æ³›åº”ç”¨äºå›¾åƒç”Ÿæˆã€æ–‡æœ¬ç”Ÿæˆã€è¯­éŸ³ç”Ÿæˆç­‰é¢†åŸŸï¼Œæˆ‘åœ¨25å¹´åˆï¼ˆå¯’å‡ï¼‰å¼€å§‹è¿›è¡Œè¿™éƒ¨åˆ†çš„å­¦ä¹ ï¼Œä¸»è¦å‚è€ƒçš„æ˜¯[Stanford CS236](https://deepgenerativemodels.github.io/syllabus.html) è¯¾ç¨‹å’Œ[MIT 6.S987](https://mit-6s978.github.io/schedule.html) è¯¾ç¨‹çš„è®²ä¹‰è¿˜æœ‰[æˆ‘å“¥çš„è‡ªå­¦ç¬”è®°](https://zhuanlan.zhihu.com/p/631001372)(ç”Ÿæˆæ¨¡å‹çš„æ¦‚è¿°ï¼ŒåŒ…å«äº†æˆªæ­¢23å¹´çš„å‡ ä¹æ‰€æœ‰æ·±åº¦ç”Ÿæˆæ¨¡å‹ğŸ‘)è¿˜æœ‰Lilian Wengçš„[åšå®¢](https://lilianweng.github.io/posts/)

    è¿›è¡Œè¿™éƒ¨åˆ†å­¦ä¹ çš„åˆè¡·æ˜¯åœ¨ä¾¯è€å¸ˆçš„å®éªŒå®¤ï¼Œæˆ‘åœ¨ç§¦ç¿å¸ˆå…„çš„æŒ‡å¯¼ä¸‹å¼€å±•å…³äºè›‹ç™½è´¨åˆ†å­ç”Ÿæˆå·¥ä½œçš„è¯„æµ‹ï¼Œç”±äºä¸æƒ³å±€é™äºä½¿ç”¨ç°æœ‰æ¨¡å‹è€Œæ˜¯èƒ½æ›´æ·±å…¥çš„ç†è§£æ¨¡å‹çš„å·¥ä½œåŸç†ï¼Œå› æ­¤å¼€å§‹è¿›è¡Œè¿™éƒ¨åˆ†çš„å­¦ä¹ ã€‚

    - [x] [VAE](./vae.md)
    - [ ] [Autoregressive Models](./ar.md)
    - [ ] [Normalizing Flows](./nf.md)
    
    - [ ] [Ganerative Adversarial Networks](./gan.md)
    - [ ] [Diffusion Models](./dm.md)


## Intro

æˆ‘ä»¬è¯´ç”Ÿæˆå¼æ¨¡å‹ï¼ˆGenerative Modelï¼‰ï¼Œä¸ä¹‹ç›¸å¯¹çš„æ˜¯åˆ¤åˆ«å¼æ¨¡å‹ï¼ˆDiscriminative Modelï¼‰ï¼š

- Discriminativeï¼š

    sample $x \rightarrow$ label $y$

    åªæœ‰ä¸€ä¸ªæœŸæœ›è¾“å‡º

- Generativeï¼š

    label $y \rightarrow$ sample $x$

    æœ‰å¤šä¸ªæœŸæœ›è¾“å‡º

???+ example "x and y"

    - åœ¨chatbotä¸­ï¼Œ$y$æ˜¯promptï¼Œ$x$æ˜¯response
    - åœ¨è›‹ç™½è´¨ç”Ÿæˆä¸­ï¼Œ$y$æ˜¯æ€§è´¨/çº¦æŸï¼Œ$x$æ˜¯è›‹ç™½è´¨ç»“æ„


![](../../assets/pics/ai/dgm/dgm1.png)

å¹¶ä¸”åˆ©ç”¨è´å¶æ–¯å…¬å¼æˆ‘ä»¬å¯ä»¥ç”±ç”Ÿæˆå¼æ¨¡å‹çš„æ¦‚ç‡å¾—åˆ°åˆ¤åˆ«å¼ï¼š

$$
p(y|x) = \frac{p(x|y)p(y)}{p(x)}
$$

ä½†æ˜¯åä¹‹å¹¶ä¸èƒ½å¾ˆå¥½çš„æˆç«‹ï¼š

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/dgm2.png" style="width: 80%;">
</div>

å› ä¸ºæˆ‘ä»¬éœ€è¦çŸ¥é“æ•°æ®sampleçš„åˆ†å¸ƒï¼Œä½†æ˜¯åªæœ‰ç”Ÿæˆå¼æ¦‚ç‡$p(x|y)$ï¼Œè€Œ$p(x)$æ˜¯ä¸çŸ¥é“çš„ã€‚

!!! note "æ€»ç»“"

    ç”Ÿæˆå¼æ¨¡å‹å°±æ˜¯è¦å»å¯»æ‰¾æ•°æ®çš„æ½œåœ¨åˆ†å¸ƒ$p(x)$ï¼Œæ¥ç”Ÿæˆä¸çœŸå®æ•°æ®åˆ†å¸ƒç›¸ä¼¼çš„æ ·æœ¬

## Probabilistic Modeling

å‰æ–‡ä¸­æˆ‘ä»¬æåˆ°äº†ä¸€ç³»åˆ—æ¦‚ç‡$p$ï¼Œä½†æ˜¯è¿™äº›æ¦‚ç‡æ˜¯å“ªé‡Œæ¥çš„å‘¢ï¼Ÿ

!!! Tip "Hint"
    <div style="text-align: center;">
        <span style="font-size: 1.5em;">
            Probability is part of the modeling.
        </span>
    </div>

æ€ä¹ˆç†è§£è¿™å¥è¯å‘¢ï¼Œæˆ‘çš„ç†è§£å°±æ˜¯æˆ‘ä»¬åœ¨å­¦ä¹ æ—¶ï¼Œå…¶å®å°±æ˜¯åœ¨å­¦ä¹ ä¸€ç§æ¦‚ç‡åˆ†å¸ƒï¼Œå¯¹è§‚æµ‹çš„æ•°æ®è¿›è¡Œå»ºæ¨¡ï¼Œæ‰€ä»¥å…¶å®å¾—åˆ°çš„åˆ†å¸ƒå‡½æ•°å°±æ˜¯æˆ‘ä»¬çš„æ¨¡å‹ã€‚

è¿™æ ·è¯´æœ‰ç‚¹æŠ½è±¡ï¼Œè¯•ç€ä¸¾ä¸ªä¾‹å­ï¼š

æˆ‘ä»¬ä»¥ç”Ÿæˆå¼æ¨¡å‹ä¸ºä¾‹ï¼Œé‡‡ç”¨æ¦‚ç‡å»ºæ¨¡çš„æ–¹æ³•ï¼š

???+ example "Image Generation"

    - æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡ç»™å®šçš„ä¸€äº›å›¾åƒï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„å›¾åƒ

    ![](../../assets/pics/ai/dgm/dgm3.png)
    
    - åœ¨é€šè¿‡ä¸€ç³»åˆ—çš„æ–¹æ³•ï¼Œå¾—åˆ°ä¸€ä¸ªä¼°è®¡ï¼ˆestimatedï¼‰çš„åˆ†å¸ƒï¼Œè¿™ä¸ªåˆ†å¸ƒçš„è¯„ä¼°æ˜¯é€šè¿‡æŸå¤±å‡½æ•°$L$æ¥è¿›è¡Œçš„
  
    ![](../../assets/pics/ai/dgm/dgm4.png)

    - æ­¤æ—¶æˆ‘ä»¬ä¾ç…§ç»™å®šç‰¹å¾$y$ï¼Œç”Ÿæˆä¸€ä¸ªå›¾åƒ$x'$ï¼Œè¿™ä¸ª$x'$å°±éµä»æˆ‘ä»¬å¾—åˆ°çš„åˆ†å¸ƒ$p(x|y)$

    ![](../../assets/pics/ai/dgm/dgm5.png)

!!! note "Notes"
    
    - Generative models involve statistical models which are often designed and derived by humans.
    - Probabilistic modeling is not just work of neural nets.
    - Probabilistic modeling is a popular way, but not the only way.




## "Deep" Generative Models

æ·±åº¦å­¦ä¹ æ˜¯ä¸€ç§è¡¨å¾å­¦ä¹ ï¼Œä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬å­¦ä¹ çš„æ˜¯å¦‚ä½•å°†æ•°æ® $x$ æ˜ å°„åˆ°$f(x)$ï¼Œä½¿å¾—æŸå¤±å‡½æ•°$L(f(x), y)$æœ€å°ã€‚

åœ¨æ·±åº¦ç”Ÿæˆæ¨¡å‹çš„å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ çš„æ˜¯å¦‚ä½•è¡¨å¾æ¦‚ç‡åˆ†å¸ƒ

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/dgm6.png" style="width: 80%;">
</div>

è¿™é‡Œæˆ‘ä»¬å­¦ä¹ å¾—åˆ°ä¸€ä¸ªç®€å•åˆ†å¸ƒåˆ°å¤æ‚åˆ†å¸ƒçš„æ˜ å°„ï¼Œè¿™ä¸ªæ˜ å°„å°±æ˜¯æˆ‘ä»¬çš„æ¨¡å‹ã€‚

$$
 \pi \rightarrow g(\pi)
$$

åƒè¿™æ ·ï¼š

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/dgm7.png" style="width: 80%;">
</div>

éšåæœ€å°åŒ–åŸºäºæ•°æ®çš„æŸå¤±å‡½æ•°$L(p_x , g(\pi))$ï¼Œå¾—åˆ°æ¨¡å‹$g$

!!! note "æ€»ç»“"

    ä¸€ä¸ªDGMå¯èƒ½åŒ…æ‹¬ï¼š

    - Formulationï¼š
        - formulate a problem as a probabilistic modeling ï¼ˆè¿›è¡Œæ¦‚ç‡å»ºæ¨¡ï¼‰
        - decompose a complex distribution into simpler and tractable ones ï¼ˆå°†å¤æ‚åˆ†å¸ƒåˆ†è§£ï¼‰
    - Representationï¼šdeep neural networks to represent data and their distributions ï¼ˆä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¡¨ç¤ºæ•°æ®å’Œä»–ä»¬çš„åˆ†å¸ƒï¼‰
    - Objective functionï¼ševaluate the predicted distribution
    - Optimizationï¼šoptimize the networks and/or the decomposition
    - Inference:
        - sampler 
        - probability density estimator
        - ...

## Latent Variable Models

åœ¨è¿™é‡Œæˆ‘è§‰å¾—éœ€è¦æå‰è¯´æ˜çš„æ˜¯ï¼Œåœ¨æ·±åº¦ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä½¿ç”¨éšå˜é‡ï¼ˆLatent Variablesï¼‰

ä¾‹å¦‚è¿™æ ·ä¸€ç»„å›¾ç‰‡ï¼š

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/dgm8.png" style="width: 50%;">
</div>

ç”±äºæ€§åˆ«ã€å¹´é¾„ã€è‚¤è‰²ç­‰ç­‰å› ç´ ï¼Œå›¾åƒ$x$å­˜åœ¨å¤šç§å¯èƒ½çš„å˜åŒ–ï¼Œä½†é™¤éå›¾ç‰‡æ˜¯è¢«æ³¨é‡Šannotatedçš„ï¼Œå¦åˆ™æˆ‘ä»¬æ— æ³•å¾—çŸ¥è¿™äº›å› ç´ ï¼Œæˆ–è€…è¯´ï¼Œè¿™äº›å› ç´ æ˜¯ä¸å¯è§çš„ï¼ˆnot explicitly availableï¼‰ã€‚

!!! tip "Latent Variable"
    è¿™æ—¶æˆ‘ä»¬çš„æƒ³æ³•å°±æ˜¯ç”¨éšå˜é‡$z$æ˜¾ç¤ºçš„å»ºæ¨¡æ¥è¡¨ç¤ºè¿™äº›å› ç´ ã€‚


å¾ˆç›´è§‚çš„æƒ³æ³•æ˜¯ç”¨ Bayes ç½‘ç»œæ¥è¡¨è¾¾ï¼š

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/dgm9.png" style="width: 80%;">
</div>

ä½†æ˜¯è¿™å…¶ä¸­çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒæ˜¯å¾ˆéš¾å¾—åˆ°çš„ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼è¿™ä¸ªæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼š

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/dgm10.png" style="width: 80%;">
</div>  

è¿™æ—¶æˆ‘ä»¬é€šå¸¸å‡è®¾$z$æ˜¯æœä»æŸç§ç®€å•åˆ†å¸ƒçš„ï¼Œä¾‹å¦‚é«˜æ–¯åˆ†å¸ƒï¼š$z \sim N(0, 1)$

é€šè¿‡ç¥ç»ç½‘ç»œçš„å¤„ç†ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°$p(x|z)=N(x|\mu_{\theta}(z), \sigma_{\theta}(z))$ï¼Œåœ¨è¿™é‡Œ$\theta$æ˜¯ç¥ç»ç½‘ç»œçš„å‚æ•°ã€‚

<div style="text-align: center;">
    <img src="/../../../../assets/pics/ai/dgm/dgm11.png" style="width: 60%;">
</div>

æˆ‘ä»¬å¸Œæœ›åœ¨è®­ç»ƒç»“æŸåï¼Œ$z$å¯ä»¥è¡¨ç¤º$x$çš„æ½œåœ¨å› ç´ ï¼ˆç‰¹å¾ï¼‰

ä½¿ç”¨$z$æ¥è¡¨ç¤ºæœ‰ä¸¤ç‚¹åŸå› ï¼š

1. ç”¨æ½œå˜é‡å¯ä»¥ç®€åŒ–é—®é¢˜
2. å¾—åˆ°çš„æ½œå˜é‡æœ¬èº«å°±å…·æœ‰ä»–çš„æ„ä¹‰ï¼ˆä¸ç”¨åœ¨ç”Ÿæˆï¼Œç”¨åœ¨æ¨æ–­æˆ–å¯»æ‰¾ç‰¹å¾ï¼‰


## Measure Distribution

åœ¨å¾—åˆ°ç”Ÿæˆåˆ†å¸ƒåï¼Œæˆ‘ä»¬éœ€è¦è¯„ä¼°è¿™ä¸ªåˆ†å¸ƒçš„ä¼˜åŠ£ï¼Œè¿™é‡Œå¯¹å‡ ä¸ªå¸¸ç”¨çš„è¯„ä¼°æ–¹æ³•è¿›è¡Œä»‹ç»ã€‚

### KL divergence

KL æ•£åº¦ï¼ˆKullback-Leibler divergenceï¼‰ï¼Œç”¨äºè¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œå¯¹äºç»™å®šçš„ä¸¤ä¸ªåˆ†å¸ƒ$p$å’Œ$q$ï¼ŒKLæ•£åº¦å®šä¹‰ä¸ºï¼š

ç¦»æ•£çš„ï¼š

$$
D_{KL}(p||q) = \sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)}
$$

è¿ç»­çš„ï¼š

$$
D_{KL}(p||q) = \int p(x) \log \frac{p(x)}{q(x)} dx
$$

- éè´Ÿæ€§ï¼š$D_{KL}(p||q) \geq 0$

- KLæ•£åº¦è¶Šå°ï¼Œä¸¤ä¸ªåˆ†å¸ƒè¶Šç›¸ä¼¼ã€‚

- KLæ•£åº¦ä¸å…·æœ‰å¯¹ç§°æ€§ï¼Œå³$D_{KL}(p||q) \neq D_{KL}(q||p)$



åœ¨å¯¹$p_{\theta}$å’Œ$p_{data}$è¿›è¡Œæ¯”è¾ƒæ—¶ï¼Œæˆ‘ä»¬å–è¿™ä¸¤ä¸ªå˜é‡çš„KLæ•£åº¦æœ€å°å€¼ï¼š

$$
\min_{\theta} D_{KL}(p_{data}||p_{\theta})
$$

å¯ä»¥æ¨å¯¼KLæ•£åº¦æœ€å°å€¼ç­‰ä»·äºæœ€å¤§åŒ– ==æœŸæœ›å¯¹æ•°ä¼¼ç„¶==ï¼ˆMaximum Log-Likelihood Estimationï¼‰ï¼šï¼ˆç”¨ç¦»æ•£çš„KLå…¬å¼ï¼Œæ‰“å¼€$\log$ï¼Œå…¶ä¸­$p_{data}(x)$æ˜¯å¸¸æ•°ï¼Œæ‰€ä»¥å¯ä»¥å¿½ç•¥ï¼‰

$$
arg\min_{\theta} D_{KL}(p_{data}||p_{\theta}) = arg\max_{\theta} \mathbb{E}_{x \sim p_{data}} \log p_{\theta}(x)
$$

!!! warning "ç¼ºé™·"

    - ç”±äºæˆ‘ä»¬å¿½ç•¥äº†$p_{data}(x)$çš„æœŸæœ›é¡¹ï¼Œæ‰€ä»¥æœ€ç»ˆæˆ‘ä»¬åªèƒ½å¾—åˆ°ä¸€ä¸ªå‚æ•°$\theta$çš„å–å€¼çš„ä¼°è®¡ï¼Œä½†æ˜¯ä¸èƒ½çŸ¥é“how close the model is to the true distributionã€‚
    - In practice, we can't compute the true distribution $p_{data}$


å¯¹äºç¬¬äºŒä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¸ºäº†é¿å…æ¶‰åŠåˆ°$p_{data}$ï¼Œæˆ‘ä»¬ä½¿ç”¨å¦ä¸€ç§ä¼¼ç„¶æ–¹æ³•ï¼š==ç»éªŒå¯¹æ•°ä¼¼ç„¶==ï¼ˆEmpirical Log-Likelihoodï¼‰

æœŸæœ›å¯¹æ•°ä¼¼ç„¶æ˜¯å¯¹æ‰€æœ‰æ•°æ®ç‚¹çš„å¯¹æ•°ä¼¼ç„¶æ±‚æœŸæœ›ï¼Œè€Œç»éªŒå¯¹æ•°ä¼¼ç„¶æ˜¯å¯¹æ‰€æœ‰æ•°æ®ç‚¹çš„å¯¹æ•°ä¼¼ç„¶æ±‚å¹³å‡ã€‚

$$
\max_{\theta} \mathbb{E}_{x \sim p_{data}} \log p_{\theta}(x) \approx \max_{p_{\theta}} \frac{1}{N} \sum_{i=1}^{N} \log p_{\theta}(x_i)
$$

è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†æ‰€éœ€è¦çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ã€‚

å¯¹äºæ•°æ®çš„æœ€å¤§åŒ–ä¼¼ç„¶ï¼šï¼ˆè”åˆæ¦‚ç‡ä¼¼ç„¶ï¼‰

$$
p_{\theta}(x^{1}, x^{2}, ..., x^{N}) = \prod_{i=1}^{N} p_{\theta}(x^{i})
$$







